services:
  inference-node:
    image: ghcr.io/product-science/mlnode:3.0.4-alpha2
    volumes:
      - ${HOME}/cache:/root/.cache
    environment:
      - HF_HOME=/root/.cache
      - HF_HUB_ENABLE_HF_TRANSFER=true
    ports:
      - "${PORT:-8000}:8080" # PoC port
      - "5000:5000" # inference port
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    command: >
      uvicorn api.app:app
      --host=0.0.0.0
      --port=8080
    restart: always
