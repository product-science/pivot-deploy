services:
 inference-node:
  image: gcr.io/decentralized-ai/mlnode:debug
  volumes:
    - ${HOME}/cache:/root/.cache
  environment:
    - HF_HOME=/root/.cache
    - HF_HUB_ENABLE_HF_TRANSFER=true
  ports:
    - "8000:8080" # PoC port
    - "5000:5000" # inference port
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: all
            capabilities: [gpu]
  command: >
    uvicorn pow.app:app
    --host=0.0.0.0
    --port=8080
  restart: always