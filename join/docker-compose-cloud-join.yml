version: "3.7"

services:
  node:
    container_name: node
    image: gcr.io/decentralized-ai/inferenced-join
    command: ["sh", "./init-docker.sh"]
    volumes:
      - .inference:/root/.inference
      - ./${GENESIS_FILE}:/root/genesis.json
    environment:
      - SEEDS=${SEEDS}
      - KEY_NAME=${KEY_NAME}
    ports:
      - "26656:26656" #p2p
      - "26657:26657" #rpc
  api:
    container_name: api
    image: gcr.io/decentralized-ai/api-join
    depends_on:
      - node
    environment:
      - KEY_NAME=${KEY_NAME}
    ports:
      - "8080:8080"
    volumes:
      - .inference:/root/.inference

  inference-node:
    image: gcr.io/decentralized-ai/vllm:0.5.0.post1
    volumes:
      - inference-data:/data
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
      - VLLM_LOGGING_LEVEL=INFO
      - VLLM_CACHE_ROOT=/data/cache
      - VLLM_ASSETS_ROOT=/data/assets
    ports:
      - "8000:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ "${DOCKER_GPU_IDS:-all}" ]
              capabilities: [ gpu ]
    command: ["--model", "unsloth/llama-3-8b", "--enforce-eager"]

volumes:
  inference-data:
