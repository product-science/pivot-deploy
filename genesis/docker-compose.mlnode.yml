services:
  inference:
    image: ghcr.io/product-science/mlnode:3.0.7
    restart: always
    command: >
      uvicorn api.app:app
      --host=0.0.0.0
      --port=8080
    ports:
      - "8080:8080"
      - "5000:5000"
    environment:
      - HF_HOME=/root/.cache
      - VLLM_ATTENTION_BACKEND=FLASHINFER
    volumes:
      - /srv/dai/cache:/root/.cache
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
